{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee5b1d-284e-4425-b633-944151539da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp core\n",
    "\n",
    "# pip install nb_black nbdev\n",
    "%load_ext lab_black\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe616e3c-e480-47d9-adb9-94a3f951a4c7",
   "metadata": {},
   "source": [
    "# Welcome\n",
    "\n",
    "This package contains a variety of building-blocks for constructing \"deliverables\" for the field of Search Engine Optimization (SEO). The goal is to make expressing such deliverables \"light and breezy\". For example, to crawl 1-page of a site into a local database:\n",
    "\n",
    "    from sqlitedict import SqliteDict as sqldict\n",
    "    import requests\n",
    "    \n",
    "    url = 'https://mikelev.in/'\n",
    "    with sqldict('crawl.db') as db:\n",
    "        db[url] = httpx.get(url)\n",
    "        db.commit()\n",
    "\n",
    "## Working In-Package (git clone)\n",
    "\n",
    "You can git clone https://github.com/miklevin/mlseo This is the Jupyter Notebook from which the pip-installable mlseo package is \"extracted\" using the nbdev tool. If you want to work directly in this Notebook you must to execute both cells with \"# export\" comment at the top. \n",
    "\n",
    "## Working In mlseo Repo (git clone)\n",
    "\n",
    "It's cleaner to work in another Notebook side-by-side in this same \"repo\" allowing you to alter the mlseo package itself locally by editing 00_core.ipynb and using nbdev_build_lib to extract new package.\n",
    "\n",
    "## Working Anywhere (pip install mlseo)\n",
    "\n",
    "And finally you can just pip install mlseo without need for this repo folder at all and use it wherever you like. It is still recommended that you start out in Jupyter and then move refined automations to headless Linux.\n",
    "\n",
    "### Jupyter\n",
    "\n",
    "For now standalone Jupyter has to be reinstalled a lot and its easy to lose your pip-installed packages. For mlseo you can get all the necessary packages back by just typing this into a Code cell:\n",
    "\n",
    "    pip install -U mlseo\n",
    "\n",
    "I also recommend installing nbdev and nb_black if you're doing any development work inside Jupyter:\n",
    "\n",
    "    pip install nb_black\n",
    "    pip install nbdev\n",
    "    \n",
    "And lastly, shortcuts always get deleted between Jupyter reinstalls so here's my most important shortcut. It's always a good time to Restart kernel and clear all outputs.\n",
    "\n",
    "    {\n",
    "        \"shortcuts\": [\n",
    "            {\n",
    "                \"command\": \"kernelmenu:restart-and-clear\",\n",
    "                \"keys\": [\n",
    "                    \"Ctrl Shift R\"\n",
    "                ],\n",
    "                \"selector\": \"body\"\n",
    "            }\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb5ecd-3c0e-4769-a03e-255cf7ec689d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Continue running the cells.</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import httpx\n",
    "import ohawf\n",
    "import pickle\n",
    "import sqlite3\n",
    "import asyncio\n",
    "import configparser\n",
    "from os import name\n",
    "import pandas as pd\n",
    "from sys import path\n",
    "from os import environ\n",
    "from art import text2art\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from asyncio import gather\n",
    "from subprocess import call\n",
    "from itertools import cycle\n",
    "from pyppeteer import launch\n",
    "from rich.table import Table\n",
    "from rich.theme import Theme\n",
    "from time import time, sleep\n",
    "from contextlib import closing\n",
    "from tldextract import extract\n",
    "from collections import Counter\n",
    "from rich.console import Console\n",
    "from nltk.corpus import stopwords\n",
    "from yake import KeywordExtractor\n",
    "from collections import namedtuple\n",
    "from nltk.stem import PorterStemmer\n",
    "from datetime import date, timedelta\n",
    "from inspect import signature, getdoc\n",
    "from apiclient.discovery import build\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from sqlitedict import SqliteDict as sqldict\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from google.analytics.admin import AnalyticsAdminServiceClient\n",
    "from google.analytics.admin_v1alpha.types import ListPropertiesRequest\n",
    "\n",
    "\n",
    "# Create do-nothing functions for running from Terminal\n",
    "# These will all be overwritten by IPython import below\n",
    "display = lambda x: x\n",
    "Markdown = lambda x: x\n",
    "Audio = lambda x: x\n",
    "first_run = True\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "try:\n",
    "    from IPython.display import display, Markdown, Audio, HTML\n",
    "\n",
    "    is_jupyter = True\n",
    "except:\n",
    "    is_jupyter = False\n",
    "\n",
    "if is_jupyter:\n",
    "    from IPython.display import display, Markdown, Audio, HTML\n",
    "\n",
    "if is_jupyter and __name__ == \"__main__\":\n",
    "    display(HTML(\"<h3>Continue running the cells.</h3>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afac54-7cfc-4cc6-b58f-02969b898b63",
   "metadata": {},
   "source": [
    "# Main Package\n",
    "\n",
    "The above and below cells constitute the entire mlseo package. You can tell because they are the only cells with the **# export** comment at the top. Whereas the above cell just imports a bunch of stuff, the below cell actually uses that stuff in building a bunch of custom functions. Everything other than the above and below cells is just example code (***using*** the mlseo package), documentation and development tools.\n",
    "\n",
    "### Continue running the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fffa9-c5af-4128-b952-5d67073273db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# mlseo: Pythonic SEO in JupyterLab"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## To begin: run_me()"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "bs = \"\\\\\"\n",
    "lr = \"\\n\"  # Because f-strings don't support \\n.\n",
    "SPACES = re.compile(r\"(?a:\\s+)\")  # RegEx pattern used to format messages\n",
    "LINES = re.compile(r\"(?a: +)\")  # RegEx pattern used to format messages\n",
    "\n",
    "# Keyword parsing stuff\n",
    "pstem = PorterStemmer()\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# For using tuples as single-column SQLite database keys\n",
    "pkl = lambda x: pickle.dumps(x)\n",
    "unpkl = lambda x: pickle.loads(x)\n",
    "\n",
    "# Connections to mlseo's enabled-by-default Google Services\n",
    "svc_ga = lambda: build(\"analytics\", \"v3\", credentials=creds)\n",
    "svc_ga4 = lambda: build(\"analyticsreporting\", \"v4\", credentials=creds)\n",
    "svc_gsc = lambda: build(\"searchconsole\", \"v1\", credentials=creds)\n",
    "svc_mail = lambda: build(\"gmail\", \"v1\", credentials=creds)\n",
    "svc_oauth = lambda: build(\"oauth2\", \"v2\", credentials=creds)\n",
    "svc_sheet = lambda: build(\"sheets\", \"v4\", credentials=creds)\n",
    "svc_photo = lambda: build(\"photoslibrary\", \"v1\", credentials=creds)\n",
    "svc_youtube = lambda: build(\"youtube\", \"v3\", credentials=creds)\n",
    "\n",
    "\n",
    "async def async_get_responses(reqs, func=None):\n",
    "    \"\"\"Return a list of tuples of asyncronously fetched URLs responses or function output.\n",
    "    Tuples always contain the key (URL or tuple) in first postion and response in second.\n",
    "    Sets up responses for rapid commit to key/value databases like SqliteDict.\"\"\"\n",
    "\n",
    "    rv = False\n",
    "\n",
    "    if not func and not callable(func):\n",
    "        if type(reqs) == str:\n",
    "            reqs = [reqs]  # Must be in list.\n",
    "        if not all([good_url(x) for x in reqs]):\n",
    "            return \"All URLs must be good\"\n",
    "\n",
    "    limits = httpx.Limits(max_keepalive_connections=5, max_connections=100)\n",
    "    async with httpx.AsyncClient(timeout=10.0, limits=limits) as client:\n",
    "        if func and callable(func):\n",
    "            use_func = func\n",
    "        else:\n",
    "            use_func = client.get\n",
    "        resps = await gather(*[use_func(req) for req in reqs])\n",
    "        rv = list(zip(reqs, resps))\n",
    "\n",
    "    return rv\n",
    "\n",
    "\n",
    "def beep():\n",
    "    \"\"\"Causes a beep when run from Jupyter. Useful when running long-running scripts\n",
    "    and you want to walk away and be alerted with a beep when the job is done.\"\"\"\n",
    "    if is_jupyter:\n",
    "        display(Audio(\"beep.wav\", autoplay=True))\n",
    "    else:\n",
    "        print(\"BEEP!\")\n",
    "\n",
    "\n",
    "def fig(text):\n",
    "    \"\"\"Returns Figlet-style ASCII-art of input text for when h1's aren't big enough.\"\"\"\n",
    "    if is_jupyter:\n",
    "        display(\n",
    "            HTML(\n",
    "                f'<pre style=\"white-space: pre;\">{text2art(text).replace(lr, \"<br/>\")}</pre>'\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(text2art(text))\n",
    "    global first_run\n",
    "    if first_run and text != \"This?\":\n",
    "        first_run = False\n",
    "        h1(\"Well done!\")\n",
    "        h2(\"You'll be an SEO in no time.\")\n",
    "        msg = f\"\"\"From this point on you will have to enter multiple lines of text for\n",
    "        each example so that it can prompt you to the next step.\"\"\"\n",
    "        print(SPACES.sub(\" \", msg))\n",
    "        msg2 = f\"\"\"\n",
    "    import httpx\n",
    "    \n",
    "    url = \"https://mikelev.in/\"\n",
    "    response = httpx.get(url)\n",
    "    \n",
    "    enlighten_me()\n",
    "    \"\"\"\n",
    "        print(msg2)\n",
    "\n",
    "\n",
    "def h1(text):\n",
    "    \"\"\"Return text as an HTML-style h1 headline when run from Jupyter.\"\"\"\n",
    "    if is_jupyter:\n",
    "        display(Markdown(f\"# {text}\"))\n",
    "    else:\n",
    "        print(f\"# {text}\")\n",
    "\n",
    "\n",
    "def h2(text):\n",
    "    \"\"\"Return text as an HTML-style h2 headline when run from Jupyter.\"\"\"\n",
    "    if is_jupyter:\n",
    "        display(Markdown(f\"## {text}\"))\n",
    "    else:\n",
    "        print(f\"## {text}\")\n",
    "\n",
    "\n",
    "def h3(text):\n",
    "    \"\"\"Return text as an HTML-style h3 headline when run from Jupyter.\"\"\"\n",
    "    if is_jupyter:\n",
    "        display(Markdown(f\"### {text}\"))\n",
    "    else:\n",
    "        print(f\"### {text}\")\n",
    "\n",
    "\n",
    "def h4(text):\n",
    "    \"\"\"Return text as an HTML-style h4 headline when run from Jupyter.\"\"\"\n",
    "    if is_jupyter:\n",
    "        display(Markdown(f\"#### {text}\"))\n",
    "    else:\n",
    "        print(f\"#### {text}\")\n",
    "\n",
    "\n",
    "def h5(text):\n",
    "    \"\"\"Return text as an HTML-style h5 headline when run from Jupyter.\"\"\"\n",
    "    if is_jupyter:\n",
    "        display(Markdown(f\"##### {text}\"))\n",
    "    else:\n",
    "        print(f\"##### {text}\")\n",
    "\n",
    "\n",
    "def h6(text):\n",
    "    \"\"\"Return text as an HTML-style h6 headline when run from Jupyter.\"\"\"\n",
    "    if is_jupyter:\n",
    "        display(Markdown(f\"####### {text}\"))\n",
    "    else:\n",
    "        print(f\"###### {text}\")\n",
    "\n",
    "\n",
    "def from_file(file_name):\n",
    "    \"\"\"Return Python list loaded from lines in file (load keywords, sites, etc).\n",
    "    Makes loading lists of keywords and URLs from file very easy.\"\"\"\n",
    "\n",
    "    rv = False\n",
    "    with open(file_name) as fh:\n",
    "        spotty = fh.read().split(\"\\n\")\n",
    "    spotless = [x for x in spotty if x]\n",
    "    if all(spotless):\n",
    "        rv = spotless\n",
    "    return rv\n",
    "\n",
    "\n",
    "def good_url(url):\n",
    "    \"\"\"Return input URL if well-formed per urlparse which evals True, Else False.\n",
    "    Makes checking whether you have a good URL very easy. Often used in all() func.\"\"\"\n",
    "\n",
    "    rv = False  # Default is to return false.\n",
    "    pieces = urlparse(url)\n",
    "    if (\n",
    "        pieces.scheme  # Notice use of short-circuit evaluation.\n",
    "        and pieces.scheme in [\"http\", \"https\"]\n",
    "        and pieces.netloc\n",
    "        and \".\" in pieces.netloc\n",
    "    ):\n",
    "        rv = url\n",
    "    return rv\n",
    "\n",
    "\n",
    "def user_agent():\n",
    "    \"\"\"Return a user-agent at random from external file of user-agents.\n",
    "    Useful for lightweight disguising of http-fetching mechanism.\"\"\"\n",
    "\n",
    "    rv = False\n",
    "    try:\n",
    "        user_agents = from_file(\"./user_agents.txt\")\n",
    "        shuffle(user_agents)\n",
    "        iteragent = cycle(user_agents)\n",
    "        rv = {\"User-agent\": next(iteragent)}\n",
    "    except:\n",
    "        pass\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_apex(url):\n",
    "    \"\"\"Return the registered or apex domain of a given good URL. Else return False.\n",
    "    Makes for a good pivot-table column (group by site) or folder save location.\"\"\"\n",
    "\n",
    "    rv = False  # Default is to return false.\n",
    "    if not good_url(url):\n",
    "        url = f\"https://{url}\"\n",
    "    if good_url(url):\n",
    "        parts = extract(url)\n",
    "        apex = f\"{parts.domain}.{parts.suffix}\"\n",
    "        rv = apex\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_meta_link(html, link_name=\"canonical\"):\n",
    "    \"\"\"Return href attribute of link element where rel attribute has provided value.\n",
    "    Mainly useful for extracting canonical.\"\"\"\n",
    "\n",
    "    if good_url(html):\n",
    "        html = get(html, headers=user_agent())\n",
    "    rv = False\n",
    "    soup = bsoup(html, \"lxml\")\n",
    "    links = soup.find_all(\"link\")\n",
    "    for link in links:\n",
    "        attrs = link.attrs\n",
    "        if \"rel\" in attrs and attrs[\"rel\"][0] == link_name:\n",
    "            if \"href\" in attrs:\n",
    "                rv = attrs[\"href\"]\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_links(page, allow_offsite=False):\n",
    "    \"\"\"Return a list of on-site links from provided URL or text.\n",
    "    Useful for site-crawling. Defaults to on-site links for crawling purposes.\"\"\"\n",
    "\n",
    "    def homepage(url):\n",
    "        \"\"\"Return guessed homepage given URL.\n",
    "        Useful for building absolute links during crawl.\"\"\"\n",
    "\n",
    "        rv = False\n",
    "        parts = urlparse(url)\n",
    "        scheme, netloc = parts.scheme, parts.netloc\n",
    "        if all([scheme, netloc]):\n",
    "            rv = f\"{parts.scheme}://{parts.netloc}/\"\n",
    "        return rv\n",
    "\n",
    "    def guess_page(text):\n",
    "        \"\"\"Return guessed url of page given only text.\n",
    "        Useful for beginning crawl from HTML fetched from uknown URL.\"\"\"\n",
    "\n",
    "        rv = False\n",
    "        canonical = extract_meta_link(text, \"canonical\")\n",
    "        if good_url(canonical):\n",
    "            rv = canonical\n",
    "        return rv\n",
    "\n",
    "    rv = False\n",
    "    if good_url(page):\n",
    "        parts = urlparse(page)\n",
    "        hpage = f\"{parts.scheme}://{parts.netloc}/\"\n",
    "        text = httpx.get(page, headers=user_agent()).text\n",
    "    else:\n",
    "        hpage = homepage(guess_page(page))\n",
    "        text = page\n",
    "    soup = bsoup(text, \"html.parser\")\n",
    "    rv = True\n",
    "    if rv:\n",
    "        seen = set()\n",
    "        table = []\n",
    "\n",
    "        for i, link in enumerate(soup.find_all(\"a\")):\n",
    "            if \"href\" in link.attrs:\n",
    "                href = link.attrs[\"href\"]\n",
    "                if \":\" in href and \"//\" not in href:\n",
    "                    continue\n",
    "                if \"://\" not in href:\n",
    "                    href = urljoin(hpage, href)\n",
    "                if href == \"/\":\n",
    "                    href = hpage\n",
    "                if allow_offsite or (homepage(href)) == hpage:\n",
    "                    if \"#\" in href:\n",
    "                        href = href[: href.index(\"#\")]\n",
    "                    if href not in seen:\n",
    "                        seen.add(href)\n",
    "                        table.append(href)\n",
    "        rv = table\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_title(page):\n",
    "    \"\"\"Return title from provided URL or text. Useful for figuring out targeted keyword of page.\"\"\"\n",
    "    rv = False\n",
    "    if good_url(page):\n",
    "        page = get(page, headers=user_agent())\n",
    "    soup = bsoup(page, \"lxml\")\n",
    "    title = soup.title.string.strip()\n",
    "    title = title.strip().replace(\"\\n\", \"\")\n",
    "    rv = title\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_meta(html, meta_name=\"description\"):\n",
    "    \"\"\"Return content attribute from meta element whose name attribute has provided value.\n",
    "    Useful for extracting descriptions and other meta values for analysis.\"\"\"\n",
    "\n",
    "    rv = None\n",
    "    if good_url(html):\n",
    "        html = httpx.get(html, headers=user_agent())\n",
    "    soup = bsoup(html, \"lxml\")\n",
    "    metas = soup.find_all(\"meta\")\n",
    "    for meta in metas:\n",
    "        attrs = meta.attrs\n",
    "        if \"name\" in attrs and attrs[\"name\"] == meta_name:\n",
    "            if \"content\" in attrs:\n",
    "                rv = attrs[\"content\"]\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_meta_link(html, link_name=\"canonical\"):\n",
    "    \"\"\"Return href attribute of link element where rel attribute has provided value.\n",
    "    Mainly useful for extracting canonical.\"\"\"\n",
    "\n",
    "    if good_url(html):\n",
    "        html = httpx.get(html, headers=user_agent())\n",
    "    rv = None\n",
    "    soup = bsoup(html, \"lxml\")\n",
    "    links = soup.find_all(\"link\")\n",
    "    for link in links:\n",
    "        attrs = link.attrs\n",
    "        if \"rel\" in attrs and attrs[\"rel\"][0] == link_name:\n",
    "            if \"href\" in attrs:\n",
    "                rv = attrs[\"href\"]\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_hx(page, hx=\"h1\"):\n",
    "    \"\"\"Return headline element from provided URL or text. Defaults to H1.\"\"\"\n",
    "    rv = None\n",
    "    if good_url(page):\n",
    "        page = httpx.get(page, headers=user_agent())\n",
    "    soup = bsoup(page, \"lxml\")\n",
    "    soup_str = f\"soup.{hx}\"\n",
    "    title = eval(soup_str)\n",
    "    if title and title.string:\n",
    "        title = title.string.strip().replace(\"\\n\", \"\")\n",
    "        rv = title\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_body(html, lower=True):\n",
    "    \"\"\"Return text extracted from body element of html.\n",
    "    Useful for looking at page-content with HTML and scripts stripped out.\"\"\"\n",
    "\n",
    "    rv = False\n",
    "    soup = bsoup(html, features=\"html.parser\").get_text(separator=\" \", strip=True)\n",
    "    if lower:\n",
    "        soup = soup.lower()\n",
    "    rv = soup\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_stemmed(html):\n",
    "    \"\"\"Return stemmed text extracted from body element of html.\n",
    "    Useful as a preliminary step for keyword analysis.\"\"\"\n",
    "\n",
    "    rv = False\n",
    "    soup = bsoup(html, features=\"html.parser\").get_text(separator=\" \", strip=True)\n",
    "    stripped = re.sub(r\"[^ -~]\", \" \", soup)\n",
    "    if stripped:\n",
    "        stemmed = pstem.stem(stripped)\n",
    "        stemmed = \" \".join([x for x in stemmed.split() if x not in stop_words])\n",
    "        rv = stemmed\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_url_keywords(url, stem=False):\n",
    "    \"\"\"Return keyword extracted from URL.\n",
    "    Useful for figuring out what keywords a URL is targeting.\"\"\"\n",
    "\n",
    "    rv = False\n",
    "    if good_url(url):\n",
    "        parts = urlparse(url)\n",
    "        keywords = re.split(\"\\W+\", parts.path)\n",
    "        keywords = \" \".join([x for x in keywords if x and not x.isnumeric()])\n",
    "        if stem:\n",
    "            keywords = extract_stemmed(keywords)\n",
    "        rv = keywords\n",
    "    return rv\n",
    "\n",
    "\n",
    "def extract_keywords(response, stem=False, top=10, ke_args=(\"en\", 3, 0.75, 100, None)):\n",
    "    \"\"\"Return list of target keywords descending by score given an httpx response object.\n",
    "    Useful for figuring out what keywords page-content is targeting.\"\"\"\n",
    "\n",
    "    rv = False\n",
    "    lan, n, dedupLim, top, features = ke_args\n",
    "\n",
    "    url_keywords = \"\"\n",
    "    if type(response) == httpx.Response:\n",
    "        url = str(response.url)\n",
    "        text = response.text\n",
    "        url_keywords = extract_url_keywords(url)\n",
    "    elif type(response) == str:\n",
    "        text = response\n",
    "    kw_extractor = KeywordExtractor(\n",
    "        lan=lan, n=n, dedupLim=dedupLim, top=top, features=features\n",
    "    )\n",
    "\n",
    "    if stem:\n",
    "        title = extract_stemmed(extract_title(text))\n",
    "        description = extract_stemmed(extract_meta(text, \"description\"))\n",
    "        body = extract_stemmed(text)\n",
    "    else:\n",
    "        title = extract_title(text).lower()\n",
    "        description = extract_meta(text, \"description\")\n",
    "        if description:\n",
    "            description = description.lower()\n",
    "        else:\n",
    "            description = \"\"\n",
    "        body = extract_body(text)\n",
    "\n",
    "    meta_stuff = \" \".join([url_keywords, title, description])\n",
    "    examine_me = \" \".join([meta_stuff, body])\n",
    "    keywords = kw_extractor.extract_keywords(examine_me)\n",
    "    keywords = [x for x in keywords if len(x[0].split()) > 1][:top]\n",
    "    rv = keywords\n",
    "\n",
    "    return rv\n",
    "\n",
    "\n",
    "def gsc_guess_site(url):\n",
    "    \"\"\"Return best site-match in Google Search Console to provided URL.\"\"\"\n",
    "    gsc_sites = [x[\"siteUrl\"] for x in svc_gsc().sites().list().execute()[\"siteEntry\"]]\n",
    "    site_dict = dict([(re.split(\"https://|http://|:\", x)[1], x) for x in gsc_sites])\n",
    "    rv = None\n",
    "    try:\n",
    "        rv = site_dict[extract_apex(url)]\n",
    "    except:\n",
    "        ...\n",
    "    return rv\n",
    "\n",
    "\n",
    "def gsc_url2keyword_query(pagestart_date=None, end_date=None, days=486, days_back=4):\n",
    "    # https://developers.google.com/webmaster-tools/search-console-api-original/v3/searchanalytics/query\n",
    "\n",
    "    # Set default start and end dates\n",
    "    today = date.today()\n",
    "    if not start_date:\n",
    "        start_date = f\"{today - timedelta(days=days + days_back)}\"\n",
    "    if not end_date:\n",
    "        end_date = f\"{today - timedelta(days=days_back)}\"\n",
    "\n",
    "    # Build the request\n",
    "    request = {\n",
    "        \"dimensions\": [\"QUERY\"],\n",
    "        \"dimensionFilterGroups\": [\n",
    "            {\n",
    "                \"filters\": [\n",
    "                    {\"dimension\": \"PAGE\", \"operator\": \"EQUALS\", \"expression\": page}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"startDate\": start_date,\n",
    "        \"endDate\": end_date,\n",
    "    }\n",
    "    return request\n",
    "\n",
    "\n",
    "def gsc_url_query(url=None, start_date=None, end_date=None, days=486, days_back=4):\n",
    "    \"\"\"Return Google Search Console query to get list of keywords for a URL.\n",
    "    Return keywords for entire site if url left off.\"\"\"\n",
    "\n",
    "    # Set default start and end dates\n",
    "    today = date.today()\n",
    "    if not start_date:\n",
    "        start_date = f\"{today - timedelta(days=days + days_back)}\"\n",
    "    if not end_date:\n",
    "        end_date = f\"{today - timedelta(days=days_back)}\"\n",
    "\n",
    "    # Build the request\n",
    "    rv = {\n",
    "        \"dimensions\": [\"QUERY\"],\n",
    "        \"startDate\": start_date,\n",
    "        \"endDate\": end_date,\n",
    "    }\n",
    "    if url:\n",
    "        rv[\"dimensionFilterGroups\"] = [\n",
    "            {\n",
    "                \"filters\": [\n",
    "                    {\"dimension\": \"PAGE\", \"operator\": \"EQUALS\", \"expression\": url}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    return rv\n",
    "\n",
    "\n",
    "def gsc2df(data, dimensions=None, extras=None):\n",
    "    \"\"\"Return Pandas DataFrame given standard GSC searchanalytics query.\n",
    "    Dimension columns will be generically named if list not provided.\"\"\"\n",
    "\n",
    "    table = []\n",
    "    if \"rows\" in data:\n",
    "        for row in data[\"rows\"]:\n",
    "            keys, clicks, impressions, ctr, position = tuple(row.values())\n",
    "            new_row = keys + [clicks, impressions, ctr, position]\n",
    "            if extras and type(extras) == dict:\n",
    "                more_cols = []\n",
    "                for akey in extras:\n",
    "                    more_cols.append(extras[akey])\n",
    "                new_row = more_cols + new_row\n",
    "            table.append(new_row)\n",
    "    defaults = [\"clicks\", \"impressions\", \"ctr\", \"position\"]\n",
    "    if not dimensions:\n",
    "        dimensions = [f\"dimension_{x + 1}\" for x in range(len(keys))]\n",
    "    columns = dimensions + defaults\n",
    "    if extras and type(extras) == dict:\n",
    "        more_cols = []\n",
    "        for akey in extras:\n",
    "            more_cols.append(akey)\n",
    "        columns = more_cols + columns\n",
    "\n",
    "    rv = pd.DataFrame(table, columns=columns)\n",
    "    return rv\n",
    "\n",
    "\n",
    "def df_allout(df, name, sql=False):\n",
    "    \"\"\"Create csv, Excel and SQL output of provided name from provided df.\"\"\"\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    df.to_csv(f\"{apex}/{name}.csv\")\n",
    "    df.to_excel(f\"{apex}/{name}.xlsx\")\n",
    "    if sql:\n",
    "        with closing(sqlite3.connect(dbfile)) as conn:\n",
    "            df.to_sql(name, conn, if_exists=\"append\", index=False)\n",
    "\n",
    "\n",
    "def ga_accounts(service=None, everything=False):\n",
    "    \"\"\"Return a list of Google Analytics Accounts accessible to Google login.\"\"\"\n",
    "    if not service:\n",
    "        service = svc_ga()\n",
    "    accounts = service.management().accounts().list().execute()\n",
    "    if everything:\n",
    "        return accounts\n",
    "    else:\n",
    "        if accounts.get(\"items\"):\n",
    "            alist = []\n",
    "            for item in accounts[\"items\"]:\n",
    "                alist.append((item[\"name\"], item[\"id\"]))\n",
    "            return alist\n",
    "\n",
    "\n",
    "def ga_properties(account, service=None, everything=False):\n",
    "    \"\"\"Return a list of Google Analytics Web Properties accessible to Google login\n",
    "    given an Account ID.\"\"\"\n",
    "    if not service:\n",
    "        service = svc_ga()\n",
    "    properties = service.management().webproperties().list(accountId=account).execute()\n",
    "    if everything:\n",
    "        return properties\n",
    "    else:\n",
    "        if properties.get(\"items\"):\n",
    "            alist = []\n",
    "            for item in properties[\"items\"]:\n",
    "                alist.append((item[\"name\"], item[\"id\"]))\n",
    "            return alist\n",
    "\n",
    "\n",
    "def ga_properties_g4(account_id):\n",
    "    \"\"\"Return a list of Google Analytics G4 Accounts accessible to Google login\n",
    "    given an Account ID.\"\"\"\n",
    "    client = AnalyticsAdminServiceClient(credentials=creds)\n",
    "    results = client.list_properties(\n",
    "        ListPropertiesRequest(filter=f\"parent:accounts/{account_id}\", show_deleted=True)\n",
    "    )\n",
    "    table = []\n",
    "    for item in results:\n",
    "        pid = item.name.split(\"/\")[1]\n",
    "        name = item.display_name\n",
    "        table.append((name, pid))\n",
    "    rv = table\n",
    "    return rv\n",
    "\n",
    "\n",
    "def ga_profiles(account, property_id, service=None, everything=False):\n",
    "    \"\"\"Return a list of Google Analytics Profile IDs accessible to Google login\n",
    "    given an Account and Web Property ID. (aka Views)\"\"\"\n",
    "    if not service:\n",
    "        service = svc_ga()\n",
    "    profiles = (\n",
    "        service.management()\n",
    "        .profiles()\n",
    "        .list(accountId=account, webPropertyId=property_id)\n",
    "        .execute()\n",
    "    )\n",
    "    if everything:\n",
    "        return profiles\n",
    "    else:\n",
    "        if profiles.get(\"items\"):\n",
    "            alist = []\n",
    "            for item in profiles[\"items\"]:\n",
    "                alist.append((item[\"name\"], item[\"id\"]))\n",
    "            return alist\n",
    "\n",
    "\n",
    "def ga_everything(service=None, ga4=False):\n",
    "    \"\"\"Print and return an object containing all Accoount, Web Property and Profile\n",
    "    IDs accessible to to Google Login.\"\"\"\n",
    "    if not service:\n",
    "        service = svc_ga()\n",
    "    acts = {}\n",
    "    accounts = ga_accounts(service)\n",
    "    for account in accounts:\n",
    "        print()\n",
    "        print(\"Account: %s %s\" % account)\n",
    "        key, val = account\n",
    "        acts[account] = []\n",
    "        plist = []\n",
    "        if ga4:\n",
    "            properties = ga_properties_g4(account[1])\n",
    "            for prop in properties:\n",
    "                print(f\"{' ' * 4}Property: {prop}\")\n",
    "                plist.append(prop)\n",
    "            acts[account].append({prop: plist})\n",
    "        else:\n",
    "            properties = ga_properties(account[1], service)\n",
    "            if properties:\n",
    "                for prop in properties:\n",
    "                    print(f\"{' ' * 4}Property: {prop}\")\n",
    "                    acts[account].append({prop: plist})\n",
    "                    profiles = ga_profiles(account[1], prop[1])\n",
    "                    if profiles:\n",
    "                        plist = []\n",
    "                        for profile in profiles:\n",
    "                            print(\"%sProfile: %s\" % (\" \" * 8, profile))\n",
    "                            plist.append(profile)\n",
    "                        acts[account].append({prop: plist})\n",
    "    return acts\n",
    "\n",
    "\n",
    "def inspect_url(url, site, service=None):\n",
    "    \"\"\"Return a Google Search Console URL Inspection API response for given URL.\"\"\"\n",
    "    if not service:\n",
    "        service = svc_gsc()\n",
    "    request = {\"inspectionUrl\": url, \"siteUrl\": site, \"languageCode\": \"en-us\"}\n",
    "\n",
    "    response = service.urlInspection().index().inspect(body=request).execute()\n",
    "    # r = response['inspectionResult']\n",
    "    r = response\n",
    "    return r\n",
    "\n",
    "\n",
    "def drop_table(db, table):\n",
    "    \"\"\"Drop SQLite table.\"\"\"\n",
    "    stmt = f\"DROP TABLE IF EXISTS {table};\"\n",
    "    with closing(sqlite3.connect(f\"{db}\")) as conn:\n",
    "        conn.execute(stmt)\n",
    "    return stmt\n",
    "\n",
    "\n",
    "def pk_compositor(db, table, columns, composite_keys):\n",
    "    \"\"\"Create SQLite table with composite primary key, especially\n",
    "    useful for transforming SqliteDict key/values to rows & columns.\n",
    "    Easiest way to use is with DataFrame column slices like this:\n",
    "    pk_compositor(\"file.db\", \"table\", df.columns, df.columns[:2])\"\"\"\n",
    "\n",
    "    def lc(maybe):\n",
    "        rv = maybe\n",
    "        if type(maybe) in [tuple, list, pd.core.indexes.base.Index]:\n",
    "            rv = \", \".join(maybe)\n",
    "        return rv\n",
    "\n",
    "    line1 = f\"CREATE TABLE IF NOT EXISTS {table} (\"\n",
    "    line2 = lc(columns)\n",
    "    line3 = f\"PRIMARY KEY ({lc(composite_keys)})) WITHOUT ROWID\"\n",
    "    stmt = f\"{line1}{line2}, {line3};\"\n",
    "    with closing(sqlite3.connect(f\"{db}\")) as conn:\n",
    "        conn.execute(stmt)\n",
    "    return stmt\n",
    "\n",
    "\n",
    "def pk_inserter(db, table, df_lot):\n",
    "    \"\"\"Insert DataFrame or list of tuples into SQLite table row-by-row.\n",
    "    Useful when using composite primary key to prevent duplicates.\n",
    "    Use after pk_compositor makes table and in place of df.to_sql().\n",
    "    Skips duplicates on try/except rather than attempting Update.\"\"\"\n",
    "\n",
    "    rv = True\n",
    "    if type(df_lot) == pd.core.frame.DataFrame:\n",
    "        lot = df.to_records(index=False)\n",
    "    elif type(df_lot) == list and type(df_lot[0]) in [tuple, list]:\n",
    "        lot = df_lot\n",
    "    else:\n",
    "        rv = False\n",
    "    if rv:\n",
    "        inserted = 0\n",
    "        with closing(sqlite3.connect(f\"{db}\")) as conn:\n",
    "            for atup in lot:\n",
    "                stmt = f\"INSERT INTO {table} VALUES {atup};\"\n",
    "                try:\n",
    "                    conn.execute(stmt)\n",
    "                    conn.commit()\n",
    "                    inserted += 1\n",
    "                except:\n",
    "                    ...\n",
    "        rv = {\"submitted\": len(lot), \"inserted\": inserted}\n",
    "    return rv\n",
    "\n",
    "\n",
    "def config2dict(fname):\n",
    "    \"\"\"Return a Python dict of a 1-section .ini file.\"\"\"\n",
    "    rv = None\n",
    "    fname = f\"config/{fname}\"\n",
    "    if fname[-4:] != \".ini\":\n",
    "        fname = f\"{fname}.ini\"\n",
    "    if not Path(fname).is_file():\n",
    "        print(f\"You must make a {fname} config file for this step.\")\n",
    "    else:\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(fname)\n",
    "        section = config.sections()[0]\n",
    "        cfg = dict([x for x in config.items(section)])\n",
    "        rv = cfg\n",
    "        # for item in cfg:\n",
    "        #     print(f\"{item}: {cfg[item]}\")\n",
    "    return rv\n",
    "\n",
    "\n",
    "#\n",
    "def build_google_search_query(keyword, site=None, num=10):\n",
    "    \"\"\"Return a URL that will perform a Google search for given keyword and optional site.\n",
    "    Useful for scraping Google search results.\"\"\"\n",
    "\n",
    "    rv = False\n",
    "    try:\n",
    "        base = \"https://www.google.com/search?q=\"\n",
    "        if site:\n",
    "            keyword = f\"site:{site} {keyword}\"\n",
    "        url = f\"{base}{quote_plus(keyword)}\"\n",
    "        if num != 10 and num % 10 == 0:\n",
    "            url = f\"{url}&start={num}\"\n",
    "        rv = url\n",
    "    except:\n",
    "        pass\n",
    "    return url\n",
    "\n",
    "\n",
    "def extract_serps(text):\n",
    "    \"\"\"Return list of Google search results from provided \"raw\" SERP scrape.\n",
    "    Useful for checking whether SERPS actually collected or extracting results.\"\"\"\n",
    "\n",
    "    rv = False\n",
    "    try:\n",
    "        div_pat = re.compile('<div class=\"yuRUbf\">(.*?)</div>')\n",
    "        divs = re.findall(div_pat, text)\n",
    "        lot = []\n",
    "        for div in divs:\n",
    "            pat_url = re.compile('<a href=\"(.*?)\"')\n",
    "            url_group = re.match(pat_url, div)\n",
    "            pat_title = re.compile('<h3 class=\"LC20lb MBeuO DKV0Md\">(.*?)</h3>')\n",
    "            title_group = re.search(pat_title, div)\n",
    "            try:\n",
    "                url = url_group.groups(0)[0]\n",
    "            except:\n",
    "                url = \"\"\n",
    "            try:\n",
    "                title = title_group.groups(0)[0]\n",
    "            except:\n",
    "                title = \"\"\n",
    "            lot.append((url, title))\n",
    "        rv = lot\n",
    "    except:\n",
    "        pass\n",
    "    return rv\n",
    "\n",
    "\n",
    "async def chrome(url, headless=False):\n",
    "    Resp = namedtuple(\"Resp\", \"url, text, status_code, headers\")\n",
    "    chrome_exe = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\n",
    "    user_data = r\"%userprofile%\\AppData\\Local\\Google\\Chrome\\User Data\"\n",
    "\n",
    "    browser = await launch(\n",
    "        autoClose=False,\n",
    "        headless=headless,\n",
    "        executablePath=chrome_exe,\n",
    "        userDataDir=user_data,\n",
    "        defaultViewport=None,\n",
    "        slowMo=10,\n",
    "    )\n",
    "    # \"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\n",
    "    page = await browser.newPage()\n",
    "    await page.setViewport({\"width\": 1024, \"height\": 1024})\n",
    "    response = await page.goto(url, timeout=500000)\n",
    "    html = await page.evaluate(\"document.documentElement.outerHTML\", force_expr=True)\n",
    "    await browser.close()\n",
    "    rv = False\n",
    "    if html:\n",
    "        # rv = Resp(url, html, response.headers[\"status\"], response.headers)\n",
    "        rv = Resp(url, html, None, None)\n",
    "    #  if name == \"nt\" else ...\n",
    "    return rv\n",
    "\n",
    "\n",
    "def run_me():\n",
    "    text = r\"\"\" Welcome to _                  (\\             To chase the rabbit,\n",
    "  _ __ ___ | |___  ___  ___     \\\\_ _/(\\      run: look()\n",
    " | '_ ` _ \\| / __|/ _ \\/ _ \\      0 0 _\\)___\n",
    " | | | | | | \\__ \\  __/ (_) |   =(_T_)=     )*\n",
    " |_| |_| |_|_|___/\\___|\\___/      /\"/   (  /\n",
    "           The adventure begins! <_<_/-<__|\"\"\"\n",
    "    print(text)\n",
    "\n",
    "\n",
    "def look():\n",
    "    h2(\"Psst!\")\n",
    "    h3(\"Want big ASCII-art like...\")\n",
    "    fig(\"This?\")\n",
    "    global first_run\n",
    "    first_run = True\n",
    "    msg = \"\"\"Try running: fig(\"Hello World\")\"\"\"\n",
    "    print(SPACES.sub(\" \", msg))\n",
    "\n",
    "\n",
    "def all_docs(docs=False):\n",
    "    \"\"\"Print every publlic function and object in global namespace with their\n",
    "    API signatures. Optionally display their docstrings.\"\"\"\n",
    "    label_width = 30\n",
    "    desc_width = 95\n",
    "    rtab = Table(title=\"functions and objects in global()\", show_lines=True)\n",
    "    ct = Theme(\n",
    "        {\n",
    "            \"func\": \"bold dim cyan\",\n",
    "            \"obj\": \"bold cyan\",\n",
    "            \"doc\": \"bold dim red\",\n",
    "        }\n",
    "    )\n",
    "    rtab.add_column(\n",
    "        \"Object or Function name\",\n",
    "        justify=\"left\",\n",
    "        style=\"white\",\n",
    "        no_wrap=False,\n",
    "        width=label_width,\n",
    "    )\n",
    "    if docs:\n",
    "        rtab.add_column(\n",
    "            \"Docstring (brief description found in function itself)\",\n",
    "            justify=\"left\",\n",
    "            style=\"white\",\n",
    "            no_wrap=False,\n",
    "            width=desc_width,\n",
    "        )\n",
    "    else:\n",
    "        rtab.add_column(\n",
    "            \"Arguments (inputs)\", justify=\"left\", style=\"white\", width=desc_width\n",
    "        )\n",
    "\n",
    "    fliter_globals = [\n",
    "        \"Table\",\n",
    "        \"Console\",\n",
    "        \"In\",\n",
    "        \"Out\",\n",
    "        \"get_ipython\",\n",
    "        \"exit\",\n",
    "        \"quit\",\n",
    "        \"core\",\n",
    "        \"is_jupyter\",\n",
    "    ]\n",
    "    global_publics = [x for x in globals() if x[:1] != \"_\" and x not in fliter_globals]\n",
    "\n",
    "    for aglobal in global_publics:\n",
    "        sig = \"\"\n",
    "        obj = eval(aglobal)\n",
    "        is_obj = False\n",
    "        try:\n",
    "            sig = f\"[func]FUNCTION ARGS:[/func] {signature(obj)}\"\n",
    "        except:\n",
    "            sig = f\"[obj]OBJECT TYPE:[/obj] {type(obj)}{lr}[obj]OBJECT REPR:[/obj] {repr(obj).replace(bs+bs, bs)}\"\n",
    "            is_obj = True\n",
    "        if docs:\n",
    "            doc = getdoc(obj)\n",
    "            if doc:\n",
    "                sig = f\"{sig}{lr}{lr}[doc]DOCSTRING[/doc]{lr}{doc}\"\n",
    "            else:\n",
    "                sig = f\"{sig}\"\n",
    "        rtab.add_row(aglobal, f\"{sig}\")\n",
    "\n",
    "    fig(\"Welcome to MLSEO\")\n",
    "    h1(\"Pythonic SEO in JupyterLab\")\n",
    "    msg = \"\"\"\\nThe following is a list of ***functions*** and pre-imported ***packages*** that are \n",
    "    available to you. **FUNCTION ARGS** means what you put in function parenthesis when you call \n",
    "    them(args). **OBJECT REPR** is the string representation of an object (you can't really show it).\n",
    "    These reside in your ***globals()*** as a result of <b>from mlseo import *</b>. If you wish to \n",
    "    learn more about using them, use: **run_me(docs=True)**\\n\"\"\"\n",
    "    if is_jupyter:\n",
    "        display(Markdown(SPACES.sub(\" \", msg)))\n",
    "    else:\n",
    "        print(SPACES.sub(\" \", msg))\n",
    "    print()\n",
    "\n",
    "    table_width = label_width + desc_width\n",
    "    console = Console(theme=ct, width=table_width)\n",
    "    # console.print(rtab)\n",
    "\n",
    "\n",
    "def enlighten_me():\n",
    "    h1(\"Congratulations!\")\n",
    "    msg = \"\"\"You just fetched a webpage from the Internet. It now resides in a variable \n",
    "    called **response**. To prove to yourself that you have the webpage in computer memory,\n",
    "    try each of the following commands, ***each on their own Code block***. Output of \n",
    "    print(response.text) is the HTML of the page and will be very long. You can delete that\n",
    "    block or hide the output after you've looked at it.\"\"\"\n",
    "    display(Markdown(SPACES.sub(\" \", msg)))\n",
    "    msg2 = \"\"\" \n",
    "    print(response)\n",
    "    type(response)\n",
    "    dir(response)\n",
    "    print(response.text)\n",
    "    \n",
    "    save_me()\"\"\"\n",
    "    print(msg2)\n",
    "\n",
    "\n",
    "def save_me():\n",
    "    h1(\"Wonderful!\")\n",
    "    msg = \"\"\"It's time to put the response **object** onto the drive so you can retreive\n",
    "    it again later without re-crawling the site. Run these lines of code. You will see\n",
    "    a file named **crawl.db** appear in the same folder as your Notebook.\"\"\"\n",
    "    display(Markdown(SPACES.sub(\" \", msg)))\n",
    "    msg2 = \"\"\" \n",
    "    from sqlitedict import SqliteDict as sqldict\n",
    "    \n",
    "    with sqldict(\"crawl.db\") as db:\n",
    "        db[url] = response\n",
    "        db.commit()\n",
    "    \n",
    "    please_explain()\"\"\"\n",
    "    print(msg2)\n",
    "\n",
    "\n",
    "def please_explain():\n",
    "    h1(\"Stay here awhile. Read. Contemplate.\")\n",
    "    msg = \"\"\"Understanding what's going on above is one of the most important things you\n",
    "    will do in your Python career. Take your time. Read each point below and really\n",
    "    try to internalize it. The key things to understand are:\"\"\"\n",
    "    display(Markdown(SPACES.sub(\" \", msg)))\n",
    "    msg2 = \"\"\" \n",
    "    1. A package called <a href=\"https://pypi.org/project/sqlitedict/\">sqlitedict</a> has been pip installed on your computer as part of the mlseo dependencies.\n",
    "    1. During the import, we are ***renaming*** sqlitedict.SqliteDict to just sqldict.\n",
    "    1. The **\"with\"** keyword means Python **CLOSES** the database on the **out-dent** using the context manager which spares us from looking at some ugly try/finally code.\n",
    "    1. The **db** object is actually the SQLite database ***pretending to be a standard dict***.\n",
    "    1. This has the effect of making the dictionary **PERSISTENT** so we can get the value again without re-crawling the site.\n",
    "    1. This is often done with Python pickles, but using SQLite is **MUCH** faster.\n",
    "    1. The **string** contents of the url variable is being used as a dictionary key.\n",
    "    1. The entire httpx.Response object is being used as a dictionary value.\n",
    "    1. The changes made to the **db** object must be ***committed*** to the file.\n",
    "    1. Using SQLite for persistent dicts is beyond useful.\n",
    "    \"\"\"\n",
    "    display(Markdown(LINES.sub(\" \", msg2)))\n",
    "\n",
    "\n",
    "if is_jupyter:\n",
    "    h1(\"mlseo: Pythonic SEO in JupyterLab\")\n",
    "    h2(\"To begin: run_me()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b77956-2b49-48b0-a87f-4518fe6590c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Welcome to _                  (\\             To chase the rabbit,\n",
      "  _ __ ___ | |___  ___  ___     \\\\_ _/(\\      run: look()\n",
      " | '_ ` _ \\| / __|/ _ \\/ _ \\      0 0 _\\)___\n",
      " | | | | | | \\__ \\  __/ (_) |   =(_T_)=     )*\n",
      " |_| |_| |_|_|___/\\___|\\___/      /\"/   (  /\n",
      "           The adventure begins! <_<_/-<__|\n"
     ]
    }
   ],
   "source": [
    "run_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fdd86-5bcf-410e-8493-06b99be1a9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Psst!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Want big ASCII-art like..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space: pre;\"> _____  _      _       ___ <br/>|_   _|| |__  (_) ___ |__ \\<br/>  | |  | '_ \\ | |/ __|  / /<br/>  | |  | | | || |\\__ \\ |_| <br/>  |_|  |_| |_||_||___/ (_) <br/>                           <br/></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try running: fig(\"Hello World\")\n"
     ]
    }
   ],
   "source": [
    "look()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7ec95-a155-4406-b360-95fbd38dbee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space: pre;\"> _   _        _  _         __        __              _      _ <br/>| | | |  ___ | || |  ___   \\ \\      / /  ___   _ __ | |  __| |<br/>| |_| | / _ \\| || | / _ \\   \\ \\ /\\ / /  / _ \\ | '__|| | / _` |<br/>|  _  ||  __/| || || (_) |   \\ V  V /  | (_) || |   | || (_| |<br/>|_| |_| \\___||_||_| \\___/     \\_/\\_/    \\___/ |_|   |_| \\__,_|<br/>                                                              <br/></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Well done!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## You'll be an SEO in no time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From this point on you will have to enter multiple lines of text for each example so that it can prompt you to the next step.\n",
      "\n",
      "    import httpx\n",
      "    \n",
      "    url = \"https://mikelev.in/\"\n",
      "    response = httpx.get(url)\n",
      "    \n",
      "    enlighten_me()\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fig(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce118e-84c7-4fd4-9dcf-47d0bbb357e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Congratulations!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You just fetched a webpage from the Internet. It now resides in a variable called **response**. To prove to yourself that you have the webpage in computer memory, try each of the following commands, ***each on their own Code block***. Output of print(response.text) is the HTML of the page and will be very long. You can delete that block or hide the output after you've looked at it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "    print(response)\n",
      "    type(response)\n",
      "    dir(response)\n",
      "    print(response.text)\n",
      "    \n",
      "    save_me()\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "\n",
    "url = \"https://mikelev.in/\"\n",
    "response = httpx.get(url)\n",
    "\n",
    "enlighten_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe0349-20ac-4852-8c6f-e3a2b182bdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200 OK]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1404f-78c7-40a2-8ee2-3e82e55a6493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "httpx.Response"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "197422d8-b7c4-41a0-b9ff-f77b52706483",
   "metadata": {},
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e63d1d14-fe67-4292-a577-4b434f9bf123",
   "metadata": {},
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6ca24-6216-45b2-8db0-81d38654df68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Wonderful!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "It's time to put the response **object** onto the drive so you can retreive it again later without re-crawling the site. Run these lines of code. You will see a file named **crawl.db** appear in the same folder as your Notebook."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "    from sqlitedict import SqliteDict as sqldict\n",
      "    \n",
      "    with sqldict(\"crawl.db\") as db:\n",
      "        db[url] = response\n",
      "        db.commit()\n",
      "    \n",
      "    please_explain()\n"
     ]
    }
   ],
   "source": [
    "save_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4b5f5-7102-4eda-9e0b-f920db75454b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Stay here awhile. Read. Contemplate."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Understanding what's going on above is one of the most important things you will do in your Python career. Take your time. Read each point I make below and really try to internalize it. The key things to understand are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       " 1. A package called <a href=\"https://pypi.org/project/sqlitedict/\">sqlitedict</a> has been pip installed on your computer as part of the mlseo dependencies.\n",
       " 1. During the import, we are ***renaming*** sqlitedict.SqliteDict to just sqldict.\n",
       " 1. The **\"with\"** keyword means Python **CLOSES** the database on the **out-dent** using the context manager which spares us from looking at some ugly try/finally code.\n",
       " 1. The **db** object is actually the SQLite database ***pretending to be a standard dict***.\n",
       " 1. This has the effect of making the dictionary **PERSISTENT** so we can get the value again without re-crawling the site.\n",
       " 1. This is often done with Python pickles, but using SQLite is **MUCH** faster.\n",
       " 1. The **string** contents of the url variable is being used as a dictionary key.\n",
       " 1. The entire httpx.Response object is being used as a dictionary value.\n",
       " 1. The changes made to the **db** object must be ***committed*** to the file.\n",
       " 1. Using SQLite for persistent dicts is beyond useful.\n",
       " "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sqlitedict import SqliteDict as sqldict\n",
    "\n",
    "with sqldict(\"crawl.db\") as db:\n",
    "    db[url] = response\n",
    "    db.commit()\n",
    "\n",
    "please_explain()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57178ecb-91ca-42a9-a052-c8a6f1be94da",
   "metadata": {},
   "source": [
    "creds = ohawf.get()  # If you need Google login right away, switch this cell to Code."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5505e0b-0c50-4193-a154-57887812cf3a",
   "metadata": {},
   "source": [
    "beep()  # Add a beep to alert you at the end of a process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312d66e-d316-460d-bf0a-d8269a40497c",
   "metadata": {},
   "source": [
    "# Configuring with .ini files \n",
    "\n",
    "You can configure values right in the Notebook, parse them from the command-line or load them from external files. I like loading them from simple ***.ini*** files. You'll find default.ini in the ***config*** folder. Copy/paste it to make your own config files and change the file-reference in config2dict(\"default.ini\").\n",
    "\n",
    "### Continue running the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc2ca9-4002-42f1-9117-8f6932e9b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"Set up test URL & config vals\")\n",
    "\n",
    "main_cfg = config2dict(\"default.ini\")\n",
    "\n",
    "\n",
    "test_url = main_cfg[\"ga_homepage\"]\n",
    "apex = extract_apex(test_url)\n",
    "assert good_url(test_url)\n",
    "assert apex == \"mikelev.in\"\n",
    "Path(apex).mkdir(parents=True, exist_ok=True)\n",
    "dbfile = f\"{apex}/{apex}.db\"\n",
    "print(\"Test URL:\", test_url)\n",
    "print(\"Database:\", dbfile)\n",
    "fig(apex)\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ab912-0f9f-4b64-b312-173dff9bc8ae",
   "metadata": {},
   "source": [
    "## Output Folder\n",
    "Tabular data gets saved to CSV, XLSX and often SQLite with the df_allout() function. The output folder is named the domain. You can watch files appear in this location. The next cell pops open this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd17f51-85b1-45bf-91c7-b6ca6af4f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"Get a single HTTP response...\")\n",
    "\n",
    "msg = \"\"\"A Webpage is being fetched based on the URL in the configuration file.\n",
    "A variable named resp is being set to the object that was returned.\n",
    "<class 'httpx.Response'> represents that object.\n",
    "It currently is being held only in memory.\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "\n",
    "resp = httpx.get(test_url, headers=user_agent())\n",
    "assert type(resp) == httpx.Response\n",
    "h2(f\"resp is a {type(resp)}\")\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e58a49-8d01-4c68-95fe-8c10628d5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"Fetch links from response text...\")\n",
    "\n",
    "msg = \"\"\"This cell inspects the in-memory resp object for all web-links and puts them into \n",
    "a list called \"links\". These links are currenly only in-memory.\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "\n",
    "links = extract_links(resp.text)\n",
    "h2(f\"Links found: {len(links)}\")\n",
    "print(\"Here's the first 5:\")\n",
    "\n",
    "for link in links[:5]:\n",
    "    print(link)\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f800639-aa48-4eef-93cf-609bb07dbd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"Put single HTTP response in database...\")\n",
    "\n",
    "msg = f\"\"\"This puts the resp data on the drive. Everything goes into the {apex} folder. \n",
    "Notice how the dictionary key IS the actual URL. URLs make good dictionary keys.\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "\n",
    "with sqldict(dbfile, tablename=\"raw_crawl\", timeout=30) as db:\n",
    "    db[test_url] = resp\n",
    "    db.commit()\n",
    "assert Path(dbfile).is_file()\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "247fe6a7-9a25-4bf0-b95b-c882f55871bd",
   "metadata": {},
   "source": [
    "# WAIT! You do not need to run this. For \"old way\" demo purpose only.\n",
    "# Keep this cell set to Raw and use it to reference sequential methods.\n",
    "\n",
    "h1(\"Perform syncronous crawl (the old way)\")\n",
    "\n",
    "print(\"Upside is that it goes directly into database.\")\n",
    "start = time()\n",
    "with sqldict(dbfile, tablename=\"raw_crawl\") as db:\n",
    "    for i, link in enumerate(links):\n",
    "        print(f\"{i} \", end=\"\")\n",
    "        resp = httpx.get(link, headers=user_agent())\n",
    "        db[link] = resp\n",
    "        db.commit()\n",
    "end = time()\n",
    "print(\"\\nSeconds\", end - start)\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5e659-cef1-4a7e-b815-ef658a0eeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but much faster and not into database\n",
    "h1(\"Asyncronously crawl URLs into memory...\")\n",
    "\n",
    "msg = f\"\"\"This cell crawls all the links putting that web content into a list named vals. \n",
    "It took a fraction of the time the sequential method (in the Raw cell above) would have taken. \n",
    "vals is only in memory. We want to commit it to database ASAP...\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "\n",
    "start = time()\n",
    "vals = await async_get_responses(links)\n",
    "h2(f\"Count: {len(vals)}\")\n",
    "print(f\"Seconds: {int(time() - start)}\")\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7e157-d670-425c-96df-73c328939047",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"Commit in-memory crawl to database...\")\n",
    "\n",
    "msg = f\"\"\"This cell commits the in-memory crawl to database. The crawl of my site \n",
    "was only ~700 pages so putting them all in memory first was okay. For larger crawls\n",
    "we would want to chunk it into controlled amounts of ~1000 URLs per concurrent crawl...\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "\n",
    "start = time()\n",
    "with sqldict(dbfile, tablename=\"raw_crawl\") as db:\n",
    "    for tup in vals:\n",
    "        key = tup[0]\n",
    "        resp = tup[1]\n",
    "        db[key] = resp\n",
    "        db.commit()\n",
    "h2(f\"Seconds: {int(time() - start)}\")\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de2374-f175-4e15-a5cf-47029f3ae39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increases click-depth of crawl by one each time run.\n",
    "h2(\"Crawl all newly discovered links...\")\n",
    "\n",
    "msg = f\"\"\"Crawling an entire site is usually achieved by repeatedly running the same function\n",
    "which looks at discovered links that have not yet been crawled, and then crawls them.\n",
    "Each time you run this block, more links will be found and crawled. Eventually it's\n",
    "supposed to reach 0 New Links found, at which time the crawl is done...\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "\n",
    "start = time()\n",
    "counter = 0\n",
    "with sqldict(dbfile, tablename=\"raw_crawl\", timeout=60) as db:\n",
    "    for i, url in enumerate(db):\n",
    "        resp = db[url]\n",
    "        if resp.status_code == 200:\n",
    "            links = extract_links(resp.text)\n",
    "            not_visited = []\n",
    "            for link in links:\n",
    "                if link not in db:\n",
    "                    not_visited.append(link)\n",
    "            vals = await async_get_responses(not_visited)\n",
    "            if vals:\n",
    "                for tup in vals:\n",
    "                    key = tup[0]\n",
    "                    data = tup[1]\n",
    "                    db[key] = data\n",
    "                    db.commit()\n",
    "                    print(\".\", end=\"\", flush=True)\n",
    "                    print(len(resp.text), key, flush=True)\n",
    "                    counter += 1\n",
    "h2(f\"New Links: {counter}\")\n",
    "print(\"Seconds:\", int(time() - start))\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674ee82-d6f1-4abe-8a02-6370c9238cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"Extract error-pages from crawl...\")\n",
    "\n",
    "msg = f\"\"\"As it happens, we don't merely store the \"view-source\" HTML of the pages. We store\n",
    "the entire \"response objecet\" which includes response headers, response codes and\n",
    "more useful info. This outputs all pages with anything other than successful...\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "\n",
    "errors = []\n",
    "with sqldict(dbfile, tablename=\"raw_crawl\") as db:\n",
    "    for i, url in enumerate(db):\n",
    "        data = db[url]\n",
    "        status = data.status_code\n",
    "        if status != 200:\n",
    "            tup = (url, status)\n",
    "            errors.append(tup)\n",
    "    df = pd.DataFrame(errors, columns=[\"url\", \"error\"])\n",
    "    df_allout(df, \"crawl_errors\", sql=True)\n",
    "\n",
    "print()\n",
    "print(\"Here's 5 random errors:\")\n",
    "display(df.sample(5))\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869833f-8b91-400d-ab6e-785a074d7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"Demonstrate asyncronous functions...\")\n",
    "\n",
    "msg = f\"\"\"This step shows how you can use the same async function that did the crawl to run\n",
    "any async function that can take a tuple as an input. You feed async_get_responses()\n",
    "a list of tuples and an async function and get back a list of results...\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "\n",
    "\n",
    "async def foo(bar):\n",
    "    return sum(bar)\n",
    "\n",
    "\n",
    "tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n",
    "tuples_summed = await async_get_responses(tuples, foo)\n",
    "\n",
    "print()\n",
    "print(f\"Input: {tuples}, foo\")\n",
    "print()\n",
    "print(f\"Output: {tuples_summed}\")\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df91259-3a26-4423-8ba5-2b2642ac041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"Extract SEO fields from crawl...\")\n",
    "\n",
    "msg = f\"\"\"So far all our work has used sqldict's which keep all data as key/value pairs\n",
    "which is usually in JSON format. This step is the first that \"flattens\" hierarchial \n",
    "data in the easiest way possible. The \"table\" list is in memory and can get big...\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "\n",
    "table = []\n",
    "with sqldict(dbfile, tablename=\"raw_crawl\", timeout=60) as db:\n",
    "    for url in db:\n",
    "        resp = db[url]\n",
    "        text = resp.text\n",
    "        urlkw = extract_url_keywords(url)\n",
    "        title = extract_title(text)\n",
    "        description = extract_meta(text)\n",
    "        canonical = extract_meta_link(text)\n",
    "        h_1 = extract_hx(text)\n",
    "        h_2 = extract_hx(text, \"h2\")\n",
    "        row = (url, title, description, canonical, h_1, h_2, urlkw)\n",
    "        table.append(row)\n",
    "cols = [\"url\", \"title\", \"description\", \"canonical\", \"h1\", \"h2\", \"url_keywords\"]\n",
    "df = pd.DataFrame(table, columns=cols)\n",
    "df_allout(df, \"crawl\", sql=True)\n",
    "print()\n",
    "print(\"Displaying 5 random pages:\")\n",
    "display(df.sample(5))\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd3a65-717b-4128-8b17-f2ebdff14e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"Display data from sqlite...\")\n",
    "\n",
    "msg = f\"\"\"This Cell simply reads the just-stored \"SEO Fields\" data back out of the database\n",
    "to demonstrate re-creating an in-memory DataFrame from a SQL table...\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "\n",
    "with closing(sqlite3.connect(dbfile)) as conn:\n",
    "    qry = \"select * from crawl;\"\n",
    "    sql_query = pd.read_sql_query(qry, con=conn)\n",
    "    df_out = pd.DataFrame(sql_query)\n",
    "print()\n",
    "print(\"Displaying 5 random pages:\")\n",
    "display(df_out.sample(5))\n",
    "h2(\"Done!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17e98154-bfea-4c58-be25-5096141cf7ce",
   "metadata": {},
   "source": [
    "drop_table(dbfile, \"crawl_keywords\")  # Change from Raw if you want to start fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc76c0-5f40-450c-b07b-7df6f1444b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"Analyze keywords from site crawl...\")\n",
    "\n",
    "msg = f\"\"\"This Cell extracts the most common keywords of each page in the crawled data.\n",
    "It's fairly slow because of the processing it's doing on each page and should run in\n",
    "sequential mode because it's safer to write to SQLite without concurrency...\"\"\"\n",
    "print(SPACES.sub(\" \", msg))\n",
    "print()\n",
    "\n",
    "table = []\n",
    "with sqldict(dbfile, tablename=\"raw_crawl\") as db:\n",
    "    with sqlite3.connect(dbfile) as conn:\n",
    "        for i, url in enumerate(db):\n",
    "            print(f\"{i} \", end=\"\", flush=True)\n",
    "            resp = db[url]\n",
    "            text = resp.text\n",
    "            keywords = extract_keywords(text)\n",
    "            keywords = [\"foo\", \"bar\"]\n",
    "            df = pd.DataFrame(\n",
    "                [(url, x[0], x[1]) for x in keywords],\n",
    "                columns=[\"url\", \"keyword\", \"weight\"],\n",
    "            )\n",
    "\n",
    "            if not i:\n",
    "                columns = df.columns\n",
    "                pkeys = columns[:2]\n",
    "                pk_compositor(dbfile, \"crawl_keywords\", columns, pkeys)\n",
    "            pk_inserter(dbfile, \"crawl_keywords\", df)\n",
    "\n",
    "h3(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37903fa4-0474-4548-b873-0323bc04b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = ohawf.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f4ed9-35e0-4b29-a867-cb3e4ea622df",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc_sites = [x[\"siteUrl\"] for x in svc_gsc().sites().list().execute()[\"siteEntry\"]]\n",
    "for asite in gsc_sites:\n",
    "    print(asite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807956d0-1c62-4fb4-aa79-68a33797d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1(\"List all Google Analytics access.\")\n",
    "\n",
    "h4(\"Classic & UA\")\n",
    "everything = ga_everything()\n",
    "h4(\"G4 Analytics\")\n",
    "everything_ga4 = ga_everything(ga4=True)\n",
    "\n",
    "h3(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
