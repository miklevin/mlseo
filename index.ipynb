{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from mlseo.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlseo\n",
    "\n",
    "> Pythonic SEO in JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install mlseo --upgrade`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "Start a new Notebook, preferably in standalone JupyterLab. Then type:\n",
    "\n",
    "```python\n",
    "from mlseo import *\n",
    "```\n",
    "    \n",
    "Chase the rabbit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gist of mlseo\n",
    "\n",
    "This is NOT an SEO Software Suite of the sort that automatically sets up webserver user interfaces for you. This is completely the opposite. This package contains a grab-bag of building-blocks useful for constructing \"deliverables\" for the field of Search Engine Optimization (SEO), and tries to entice you into coding some Python in JupyterLab Desktop.\n",
    "\n",
    "## Light & Breezy Python\n",
    "\n",
    "The goal is to make expressing such deliverables \"light and breezy\" by establishing certain good Python (Pythonic) conventions that you can use throrought your data-jockying career. The approach I'm about to show you is ***perfect*** for raw-data capture from API-calls, using the arguments of the API-call itself as the database-key to recover the locally stored response. If this sounds like gobbledygook to you right now, just bear with me. You'll get it.\n",
    "\n",
    "## Best Technique You've Never Heard Of\n",
    "\n",
    "Use the exact values you fed to the API to fetch the data in the first place as the keys to your database to retreive the data again locally. I just boosted your earning-capacity x2 at least. SQLite3 (part of standard Python) is a gift. The dict API connected to it is a gift. The context-manager (\"with *something* as *something*\") is a gift. Use them.\n",
    "\n",
    "For example, to crawl 1-page of a site into a local database:\n",
    "\n",
    "```python\n",
    "import httpx\n",
    "from sqlitedict import SqliteDict as sqldict\n",
    "\n",
    "url = 'https://mikelev.in/'\n",
    "with sqldict('crawl.db') as db:\n",
    "    db[url] = httpx.get(url)\n",
    "    db.commit()\n",
    "```\n",
    "\n",
    "## Tuples As Composite-Keys (Unique Constraints)\n",
    "If the database key should also contain the **date** of the crawl and a full/partial boolean (True/False), we can use a 3-position tuple. This is better practice than appending strings together, because you can keep dates as real datetime objects and perform date operations on them easily when stepping through records.\n",
    "\n",
    "```python\n",
    "from datetime import date\n",
    "\n",
    "url = 'https://mikelev.in/'\n",
    "atuple = (date.today(), url, True)\n",
    "```\n",
    "\n",
    "### Pickling and Unpickling\n",
    "\n",
    "The way tuples become string keys (necessary for sqlitedict) is by a common Python serialization function called ***pickling***. We \"pickle\" the tuple to make it a string-based dictionary key. We can then iterate through all keys, unpickling the primary key and have it back in its orginal tuple-state as we go.\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "pkl = lambda x: pickle.dumps(x)\n",
    "unpkl = lambda x: pickle.loads(x)\n",
    "\n",
    "url = 'https://mikelev.in/'\n",
    "today = date.today()\n",
    "\n",
    "atuple = (today, url, True)\n",
    "now_a_string = pkl(atuple)\n",
    "\n",
    "\n",
    "print(now_a_string)\n",
    "b'\\x80\\x04\\x959\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08datetime\\x94\\x8c\\x04date\\x94\\x93\\x94C\\x04\\x07\\xe6\\x04\\x04\\x94\\x85\\x94R\\x94\\x8c\\x13https://mikelev.in/\\x94\\x88\\x87\\x94.'\n",
    "\n",
    "print(unpkl(now_a_string))\n",
    "(datetime.date(2022, 4, 4), 'https://mikelev.in/', True)\n",
    "```\n",
    "\n",
    "### Pickling Keys For Database\n",
    "\n",
    "The example below puts the 2 above examples together to save the page-crawl to the database using a pickled tuple as the dictionary key. This is worth contemplating. Composite primary keys are unique constraints, thus naturally preventing duplicate redords from being recorded for the same URL for the same day. This sets the stage for efficient subsequent crawls.\n",
    "\n",
    "```python\n",
    "import httpx\n",
    "import pickle\n",
    "from datetime import date\n",
    "from sqlitedict import SqliteDict as sqldict\n",
    "\n",
    "\n",
    "pkl = lambda x: pickle.dumps(x)\n",
    "unpkl = lambda x: pickle.loads(x)\n",
    "\n",
    "url = 'https://mikelev.in/'\n",
    "\n",
    "# Data goes in\n",
    "with sqldict('crawl.db') as db:\n",
    "    tupkey = (date.today(), url, True)\n",
    "    db[pkl(tupkey)] = httpx.get(url)\n",
    "    db.commit()\n",
    "\n",
    "# Data comes out\n",
    "with sqldict('crawl.db') as db:\n",
    "    for tupkey in db:\n",
    "        adate, url, full = unpkl(tupkey)\n",
    "        print(adate, url, full)\n",
    "```\n",
    "\n",
    "# From Here\n",
    "\n",
    "By following the install and how-to-use instructions above, you will be invited to run_me(), thereby initiating the example given here. Do the trick to get the trick. This is top-down education. \\*POOF\\* here's a gift. Now look at how that works.    \n",
    "\n",
    "# A Word About JupyterLab\n",
    "\n",
    "## Recovering pip installs\n",
    "\n",
    "For now standalone Jupyter has to be reinstalled a lot and its easy to lose your pip-installed packages. For mlseo you can get all the necessary packages back by just typing this into a Code cell:\n",
    "\n",
    "    pip install mlseo --upgrade\n",
    "\n",
    "## Useful Dev Tools\n",
    "\n",
    "I also recommend installing nbdev and nb_black if you're doing any development work inside Jupyter:\n",
    "\n",
    "    pip install nb_black\n",
    "    pip install nbdev\n",
    "\n",
    "## Restart Kernel & Clear All Outputs A LOT\n",
    "\n",
    "And lastly, shortcuts always get deleted between Jupyter reinstalls so here's my most important shortcut. It's always a good time to Restart kernel and clear all outputs.\n",
    "```javascript\n",
    "{\n",
    "    \"shortcuts\": [\n",
    "        {\n",
    "            \"command\": \"kernelmenu:restart-and-clear\",\n",
    "            \"keys\": [\n",
    "                \"Ctrl Shift R\"\n",
    "            ],\n",
    "            \"selector\": \"body\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
