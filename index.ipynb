{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from mlseo.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlseo\n",
    "\n",
    "> Pythonic SEO in JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install mlseo --upgrade`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "Start a new Notebook, preferably in standalone JupyterLab. Then type:\n",
    "\n",
    "```python\n",
    "from mlseo import *\n",
    "```\n",
    "\n",
    "\n",
    "     Welcome to _                  (\\         To chase the rabbit,\n",
    "      _ __ ___ | |___  ___  ___     \\\\_ _/(\\      run: look()\n",
    "     | '_ ` _ \\| / __|/ _ \\/ _ \\      0 0 _\\)___\n",
    "     | | | | | | \\__ \\  __/ (_) |   =(_T_)=     )*\n",
    "     |_| |_| |_|_|___/\\___|\\___/      /\"/   (  /\n",
    "               The adventure begins! <_<_/-<__|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Most Important Things\n",
    "\n",
    "## Storing API Responses into Database\n",
    "\n",
    "This package contains a variety of building-blocks for constructing \"deliverables\" for the field of Search Engine Optimization (SEO). The goal is to make expressing such deliverables \"light and breezy\" by establishing certain conventions. For example, to crawl 1-page of a site into a local database:\n",
    "\n",
    "```python\n",
    "import httpx\n",
    "from sqlitedict import SqliteDict as sqldict\n",
    "\n",
    "url = 'https://mikelev.in/'\n",
    "with sqldict('crawl.db') as db:\n",
    "    db[url] = httpx.get(url)\n",
    "    db.commit()\n",
    "```\n",
    "\n",
    "## Using Tuples As Composite-Keys\n",
    "\n",
    "We are using SQlite as a key-value database in a way that requires the keys to be strings. Keys must be unique so if we use the URL as the key we can store each page we crawl only once. Instead of just a URL, the key to your database can contain a URL and Date so that we can crawl sites on subsequent days and store it into the same key-value database. Such a tuple key looks like this:\n",
    "\n",
    "```python\n",
    "from datetime import date\n",
    "\n",
    "url = 'https://mikelev.in/'\n",
    "atuple = (date.today(), url)\n",
    "```\n",
    "\n",
    "## Pickling and Unpickling\n",
    "\n",
    "Tuples must become strings to be a key in the key-value database we're using. This is accomplished through ***pickling***. We \"pickle\" the tuple to make it a string, then can use that string as a key in the dictionary database.\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "from datetime import date\n",
    "\n",
    "pkl = lambda x: pickle.dumps(x)\n",
    "unpkl = lambda x: pickle.loads(x)\n",
    "\n",
    "url = 'https://mikelev.in/'\n",
    "today = date.today()\n",
    "atuple = (today, url)\n",
    "\n",
    "now_a_string = pkl(atuple)\n",
    "print(now_a_string)\n",
    "```\n",
    "\n",
    "### Pickling Keys For Database\n",
    "\n",
    "This example uses a pickled tuple containing the Date and the URL as the database key. It shows data both going in and coming out of the database. Notice the pickled key is restored to its original form. This approach prevents duplicate records in your database. Because dictionary keys must be unique, attempts to insert a new record with the same URL+Date key will fail, meaning this crawler can only record each page on the site once per day.\n",
    "\n",
    "```python\n",
    "import httpx\n",
    "import pickle\n",
    "from datetime import date\n",
    "from sqlitedict import SqliteDict as sqldict\n",
    "\n",
    "\n",
    "pkl = lambda x: pickle.dumps(x)\n",
    "unpkl = lambda x: pickle.loads(x)\n",
    "\n",
    "url = 'https://mikelev.in/'\n",
    "\n",
    "# Data goes in\n",
    "with sqldict('crawl.db') as db:\n",
    "    tupkey = (date.today(), url)\n",
    "    db[pkl(tupkey)] = httpx.get(url)\n",
    "    db.commit()\n",
    "\n",
    "# Data comes out\n",
    "with sqldict('crawl.db') as db:\n",
    "    for tupkey in db:\n",
    "        adate, url = unpkl(tupkey)\n",
    "        print(adate, url)\n",
    "```\n",
    "\n",
    "# mlseo Tutorials\n",
    "\n",
    "[**HOUSEKEEPING:**](./housekeeping.ipynb) Once you have the basic trick of using a persistent dictionary and using tuples as your primary key, you'll need a place to ***put*** the database and all your other INPUT/OUTPUT files besides lumping it all into the top-level of your folder.\n",
    "\n",
    "## A Word About JupyterLab\n",
    "\n",
    "### Recovering pip installs\n",
    "\n",
    "For now standalone Jupyter has to be reinstalled a lot and its easy to lose your pip-installed packages. For mlseo you can get all the necessary packages back by just typing this into a Code cell:\n",
    "\n",
    "    pip install mlseo --upgrade\n",
    "\n",
    "### Useful Dev Tools\n",
    "\n",
    "I also recommend installing nbdev and nb_black if you're doing any development work inside Jupyter:\n",
    "\n",
    "    pip install nb_black\n",
    "    pip install nbdev\n",
    "\n",
    "### Restart Kernel & Clear All Outputs A LOT\n",
    "\n",
    "And lastly, shortcuts always get deleted between Jupyter reinstalls so here's my most important shortcut. It's always a good time to Restart kernel and clear all outputs.\n",
    "```javascript\n",
    "{\n",
    "    \"shortcuts\": [\n",
    "        {\n",
    "            \"command\": \"kernelmenu:restart-and-clear\",\n",
    "            \"keys\": [\n",
    "                \"Ctrl Shift R\"\n",
    "            ],\n",
    "            \"selector\": \"body\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
